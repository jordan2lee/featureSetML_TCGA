{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_arguments():\n",
    "#     parser = argparse.ArgumentParser(description='')\n",
    "#     parser.add_argument(\"-t\", \"--tumor\", help =\"cancer cohort\", required=True, type=str)\n",
    "#     parser.add_argument(\"-m\", \"--metric\", help =\"classification performance metric\", required=True, type=str)\n",
    "#     parser.add_argument(\"-fil\", \"--filters\", help =\"none or integer for feature set filter max size to be considered for best model\", required=True, type=str)\n",
    "#     parser.add_argument(\"-f1\", \"--file1_fts\", help =\"classification performance file with all groups\", required=True, type=str)\n",
    "#     parser.add_argument(\"-f2\", \"--file2_perform\", help =\"feature set file with all groups\", required=True, type=str)\n",
    "#     parser.add_argument(\"-o\", \"--out\", help =\"output file\", required=True, type=str)\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = get_arguments()\n",
    "# cancer = args.tumor\n",
    "# pmetric = args.metric\n",
    "# file_fts = args.file1_fts\n",
    "# file_preds = args.file2_perform\n",
    "# file_output = args.out\n",
    "# filters = args.filters\n",
    "\n",
    "cancer = 'BRCA'\n",
    "pmetric = 'overall_weighted_f1'\n",
    "file_fts = '../../src/collected_features_matrix_20200722.tsv.gz'\n",
    "file_preds = '../../src/feature_list_with_performance_with_subtype_names_20200828.tsv.gz'\n",
    "file_output = 'data/figure_panel_a/TESTING_best_models_' + cancer + '.tsv'\n",
    "filters = 'none'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = [\n",
    "    'ACC',\n",
    "    'BLCA',\n",
    "    'BRCA',\n",
    "    'CESC',\n",
    "    'COADREAD',\n",
    "    'ESCC',\n",
    "    'GEA',\n",
    "    'HNSC',\n",
    "    'KIRCKICH',\n",
    "    'KIRP',\n",
    "    'LGGGBM',\n",
    "    'LIHCCHOL',\n",
    "    'LUAD',\n",
    "    'LUSC',\n",
    "    'MESO',\n",
    "    'OV',\n",
    "    'PAAD',\n",
    "    'PCPG',\n",
    "    'PRAD',\n",
    "    'SARC',\n",
    "    'SKCM',\n",
    "    'TGCT',\n",
    "    'THCA',\n",
    "    'THYM',\n",
    "    'UCEC',\n",
    "    'UVM'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Hardcoded Object\n",
    "groups = ['gnosis', 'CF|All', 'AKLIMATE', 'nn', ['rfe15', 'fbedeBIC']]\n",
    "#############\n",
    "\n",
    "performance_df = pd.read_csv(file_preds, sep = '\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cancer in cohorts:\n",
    "    # For a group: select best model\n",
    "    ct = 1\n",
    "    best = []\n",
    "    for group in groups:\n",
    "        # all models from that group for the 3 criteria\n",
    "        if type(group)== list:\n",
    "            subset = performance_df[performance_df['feature_list_method'].isin(['rfe15', 'fbedeBIC'])]\n",
    "            subset = subset[subset['cohort'] == cancer]\n",
    "            subset = subset[subset['performance_metric'] == pmetric].reset_index(drop=True)\n",
    "            if filters != 'none':\n",
    "                max_ft_size = int(filters)\n",
    "                subset = subset[subset['total_features'] < max_ft_size].reset_index(drop=True)\n",
    "            subset = subset.sort_values(by='Mean', ascending=False).reset_index(drop=True)\n",
    "        else:\n",
    "            print('###', group, '###')\n",
    "            subset = performance_df[performance_df['feature_list_method'] == group]\n",
    "            subset = subset[subset['cohort'] == cancer]\n",
    "            subset = subset[subset['performance_metric'] == pmetric].reset_index(drop=True)\n",
    "            if filters != 'none':\n",
    "                max_ft_size = int(filters)\n",
    "                subset = subset[subset['total_features'] < max_ft_size].reset_index(drop=True)\n",
    "            subset = subset.sort_values(by='Mean', ascending=False).reset_index(drop=True)\n",
    "        # Get table info for best model\n",
    "        tab1 = subset.sort_values(by='Mean', ascending=False).reset_index(drop=True)\n",
    "        # Grab the name of the model with highest MEAN performance metric\n",
    "        ftID = tab1['featureID'][0]\n",
    "\n",
    "        ##\n",
    "        # Fix naming of CloudForest (to match ft file)\n",
    "        if \"CF|\" in ftID:\n",
    "            ftID='Top '.join(ftID.split('Top_'))\n",
    "        ##\n",
    "\n",
    "        best.append(ftID)\n",
    "        print(ftID, ' selected as best model for group')\n",
    "\n",
    "\n",
    "        ###### additional plots\n",
    "        # Create tab for plots to use\n",
    "        subset2 = tab1[['featureID', 'cohort', 'Mean', 'performance_metric', 'model']]\n",
    "        subset2['Loss'] = abs(subset2['Mean'].diff())\n",
    "\n",
    "        # Make plots\n",
    "        if type(group)== list:\n",
    "            team = 'SciKitGrid'\n",
    "        else:\n",
    "            team = group\n",
    "        # Plot of performance of models from a given team\n",
    "        p1= plt.plot(subset2['Mean'])\n",
    "        p1= plt.xlabel('Model Rank')\n",
    "        p1= plt.ylabel('Mean {}'.format(pmetric))\n",
    "\n",
    "        positions = list(range(0, subset2.shape[0]))\n",
    "        labels = list(subset2['model'])\n",
    "        p1= plt.xticks(positions, labels)\n",
    "        p1= plt.xticks(rotation = 90)\n",
    "        plt.savefig('wip_figures/{}_performance_{}.pdf'.format(cancer, team), bbox_inches ='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # Plot of performance loss as decrease ft size of models from a given team\n",
    "        p2= plt.plot(subset2['Loss'])\n",
    "        p2= plt.xlabel('Model Rank')\n",
    "        p2= plt.ylabel('Loss of Mean {}'.format(pmetric))\n",
    "\n",
    "        positions = list(range(0, subset2.shape[0]))\n",
    "        labels = list(subset2['model'])\n",
    "        p2= plt.xticks(positions, labels)\n",
    "        p2= plt.xticks(rotation = 90)\n",
    "        plt.savefig('wip_figures/supp/{}_loss_{}.pdf'.format(cancer, team), bbox_inches ='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
