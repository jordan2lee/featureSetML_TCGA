{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197bd232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note this is PART of the code from get_model_info_ALL.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "### 1. Get performance info\n",
    "# Get top performing models\n",
    "df = pd.read_csv('../src/classifier_metrics_20220511/big_results_matrix.tsv', sep='\\t', low_memory=False)\n",
    "\n",
    "platform_options = {\n",
    "    'CF':['GEXP' , 'CNVR', 'METH', 'MUTA', 'MIR', 'All', 'OVERALL'],\n",
    "    'AKLIMATE':['GEXP' , 'CNVR', 'METH', 'MUTA', 'MIR', 'MULTI', 'TOP'], \n",
    "    'skgrid':['GEXP' , 'CNVR', 'METH', 'MUTA', 'MIR', 'OVERALL'],\n",
    "    'subSCOPE':['GEXP' , 'CNVR', 'METH', 'MUTA', 'MIR', 'OVERALL'],\n",
    "    'jadbio':['GEXP' , 'CNVR', 'METH', 'MUTA', 'MIR', 'MULTI']\n",
    "\n",
    "}\n",
    "\n",
    "groups = ['AKLIMATE', 'CF', 'jadbio', 'skgrid', 'subSCOPE']\n",
    "cancers = ['BRCA', 'LGGGBM', 'COADREAD', 'SKCM', 'ACC', 'BLCA', 'CESC', 'ESCC', 'GEA', 'HNSC', 'KIRCKICH', 'KIRP', 'LIHCCHOL', 'LUAD', 'LUSC', 'MESO', 'OV', 'PAAD', 'PCPG', 'PRAD', 'SARC', 'TGCT', 'THCA', 'THYM', 'UCEC', 'UVM']\n",
    "pmetric = 'overall_weighted_f1'\n",
    "filters = 100 # max size of ft list\n",
    "    \n",
    "best = {}\n",
    "for cancer in cancers: \n",
    "    print(cancer)\n",
    "    # For a group: select best model\n",
    "    ct = 1\n",
    "    cancer_dict = {}\n",
    "    for group in groups:\n",
    "        \n",
    "        ## uncomment out this if want to set a ftset size max##\n",
    "        # remove rows where no feature list len provided\n",
    "        # note this might be a spot need to fix later\n",
    "        df = df[df['total_features']!= '__NO_LIST__'].reset_index(drop=True)\n",
    "        # change type of col\n",
    "        df['total_features'] = df['total_features'].astype('int')\n",
    "\n",
    "        # Grabs all models for: method, cancer, overall_weighted_f1\n",
    "        subset = df[df['feature_list_method'] == group]\n",
    "        subset = subset[subset['cohort'] == cancer]\n",
    "        subset = subset[subset['performance_metric'] == pmetric].reset_index(drop=True)\n",
    "        \n",
    "        # uncomment out this if want to set a ftset size max##\n",
    "        # filter for max ft size\n",
    "        max_ft_size = int(filters)\n",
    "        subset = subset[subset['total_features'] <= max_ft_size].reset_index(drop=True)\n",
    "        subset = subset.sort_values(by='Mean', ascending=False)\n",
    "\n",
    "        # Get the model for each platform\n",
    "        platforms = platform_options[group]\n",
    "        for platform in platforms:\n",
    "            col_options = ['GEXP_features','CNVR_features','MIR_features','MUTA_features','METH_features']\n",
    "\n",
    "            # filter models specific to group\n",
    "            if group == 'CF':\n",
    "                if platform != 'OVERALL':\n",
    "                    # CF specific. featureid has info of platform at index 3\n",
    "                    # note: \"All\" is at index2 (CF_ACC_All_Top_100). must use all datatypes\n",
    "                    ftid_keep= [f for f in subset['featureID'] if f.split('_')[2]==platform]\n",
    "                    platform_subset = subset[subset['featureID'].isin(ftid_keep)].reset_index(drop=True)\n",
    "                # no filtering done.  \"OVERALL\" is the best single or multi data type model.\n",
    "                else:\n",
    "                    platform_subset = subset\n",
    "            elif group == 'AKLIMATE':\n",
    "                print(platform)\n",
    "                # AKLIMATE specific. featureid has info of platform at index 1\n",
    "                # single platform is AKLIMATE_METH_ONLY_BRCA_reduced_model_5_feature_set_BRCA\n",
    "                if platform in ['GEXP' , 'CNVR', 'METH', 'MUTA', 'MIR']:    \n",
    "                    ftid_keep= [f for f in subset['featureID'] if f.split('_')[1]==platform]\n",
    "                    platform_subset = subset[subset['featureID'].isin(ftid_keep)].reset_index(drop=True)\n",
    "                # # \"MULTI\" can use all platform is AKLIMATE_BRCA_reduced_model_100_feature_set_BRCA\n",
    "                elif platform =='MULTI':\n",
    "                    ftid_keep= [f for f in subset['model'] if f.split('_')[1]=='MULTI']\n",
    "                    platform_subset = subset[subset['model'].isin(ftid_keep)].reset_index(drop=True)\n",
    "                # no filtering done.  \"TOP\" is the best single or multi data type model.\n",
    "                elif platform == 'TOP':\n",
    "                    platform_subset = subset \n",
    "            elif group =='skgrid':\n",
    "                if platform in ['GEXP' , 'CNVR', 'METH', 'MUTA', 'MIR']:\n",
    "                    ftid_keep= [f for f in subset['featureID'] if f.split('_')[3]=='perplatform'+platform]\n",
    "                    platform_subset = subset[subset['featureID'].isin(ftid_keep)].reset_index(drop=True)\n",
    "                elif platform =='OVERALL':\n",
    "                    # no filtering done.  \"OVERALL\" is the best single or multi data type model.\n",
    "                    platform_subset = subset  \n",
    "            elif group == 'subSCOPE':\n",
    "                # note we do not have a statement for \"ENSEMBLE\" which can use multiple datatypes\n",
    "                if platform in ['GEXP' , 'CNVR', 'METH', 'MUTA', 'MIR']:\n",
    "                    if platform == 'CNVR':\n",
    "                        ftid_keep = [f for f in subset['featureID'] if f.split('_')[0]=='subSCOPE-'+platform.replace('R', '')]\n",
    "                    else:\n",
    "                        ftid_keep= [f for f in subset['featureID'] if f.split('_')[0]=='subSCOPE-'+platform]\n",
    "                    platform_subset= subset[subset['featureID'].isin(ftid_keep)].reset_index(drop=True)\n",
    "                elif platform =='OVERALL':\n",
    "                    # no filtering done. where OVERALL is the best single or multi data type model\n",
    "                    platform_subset = subset  \n",
    "            elif group == 'jadbio':\n",
    "                if platform in ['GEXP' , 'CNVR', 'METH', 'MUTA', 'MIR']:    \n",
    "                    ftid_keep= [f for f in subset['featureID'] if f.split('_')[2]==platform]\n",
    "                    platform_subset = subset[subset['featureID'].isin(ftid_keep)].reset_index(drop=True)\n",
    "                elif platform =='MULTI':\n",
    "                    ftid_keep= [f for f in subset['featureID'] if f.split('_')[2]=='MULTIDATATYPE']\n",
    "                    platform_subset = subset[subset['featureID'].isin(ftid_keep)].reset_index(drop=True)\n",
    "                elif group== 'OVERALL':\n",
    "                    # no filtering done. where OVERALL is the best single or multi data type model\n",
    "                    platform_subset = subset  \n",
    "\n",
    "\n",
    "            # Grab name of model wit ht eh highest MEWAN performance metic (overall balanced f1)\n",
    "            platform_subset = platform_subset.sort_values(by='Mean', ascending=False).reset_index(drop=True)\n",
    "            # if found at least one model\n",
    "            if platform_subset.shape[0] > 0:\n",
    "                ftID = platform_subset.sort_values(by='Mean', ascending=False).reset_index(drop=True)['featureID'][0]\n",
    "            #else no models fitting the above filters, need to finish\n",
    "            else:\n",
    "                ftID = 'NO_MODEL_MATCH_' + group\n",
    "            # note can grab 'model' col at this point too if want, it will be located at row 0\n",
    "\n",
    "            # save\n",
    "            if group not in cancer_dict:\n",
    "                cancer_dict[group] ={platform:ftID}\n",
    "                # add new extra info\n",
    "                if ftID.startswith('NO_MODEL_MATCH_'):\n",
    "                    cancer_dict[group]['info_'+platform]= {'Mean_'+pmetric: np.nan, \n",
    "                     'Std_'+pmetric: np.nan, \n",
    "                     'Max_'+pmetric:np.nan,\n",
    "                     'full_featureID': np.nan,\n",
    "                     'full_model':np.nan,\n",
    "                    }   \n",
    "                else:\n",
    "                    cancer_dict[group]['info_'+platform]= {'Mean_'+pmetric: platform_subset['Mean'][0], \n",
    "                     'Std_'+pmetric: platform_subset['Std'][0],\n",
    "                     'Max_'+pmetric: platform_subset['Max'][0],\n",
    "                     'full_featureID': platform_subset['featureID'][0],\n",
    "                     'full_model':platform_subset['model'][0],\n",
    "                    }                \n",
    "            else:\n",
    "                cancer_dict[group][platform] =ftID\n",
    "                # add new extra info\n",
    "                if ftID.startswith('NO_MODEL_MATCH_'):\n",
    "                    cancer_dict[group]['info_'+platform]= {'Mean_'+pmetric:np.nan, \n",
    "                     'Std_'+pmetric: np.nan, \n",
    "                     'Max_'+pmetric:np.nan,\n",
    "                     'full_featureID': np.nan,\n",
    "                     'full_model':np.nan,\n",
    "                    }   \n",
    "                else:\n",
    "                    cancer_dict[group]['info_'+platform]= {'Mean_'+pmetric: platform_subset['Mean'][0], \n",
    "                     'Std_'+pmetric: platform_subset['Std'][0], \n",
    "                     'Max_'+pmetric: platform_subset['Max'][0],\n",
    "                     'full_featureID': platform_subset['featureID'][0],\n",
    "                     'full_model':platform_subset['model'][0],\n",
    "                    }      \n",
    "            print(ftID, ' selected as best model for group')\n",
    "            \n",
    "    best[cancer]=cancer_dict\n",
    "\n",
    "\n",
    "\n",
    "# save this ref file \n",
    "with open('../data/table_docker_info/options_extended_100ftmax.yml', 'w') as fh:\n",
    "    yaml.dump(best, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f00d38",
   "metadata": {},
   "source": [
    "# Choice 1: blinding use christinas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe942227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # what has been added \n",
    "# # OVERALL - cloudforest\n",
    "# import pandas as pd\n",
    "# import yaml\n",
    "\n",
    "# nameconvert = {\n",
    "#     'aklimate':'AKLIMATE', \n",
    "#     'cloudforest':'CF',\n",
    "#     'jadbio':'jadbio',\n",
    "#     'skgrid':'skgrid',\n",
    "#     'subscope':'subSCOPE'\n",
    "# }\n",
    "\n",
    "# # pull up info used in docker models\n",
    "# with open('../../../09_docker/gdan-tmp-models/tools/model_info.json', 'r') as file:\n",
    "#     info = yaml.safe_load(file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# # read in christina data \n",
    "# perf_df =pd.read_csv('../src/BestModelPerDataTypePerGroup_deduplicated_2022_06_16_fixed_fromChristina.txt', sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print('TODO update to get performance info for All and MULTI and OVERALL and TOP (note in df it shows All not ALL)')\n",
    "# for algor in info.keys():\n",
    "#     for cancer in info[algor].keys():\n",
    "#         for platform in info[algor][cancer].keys():\n",
    "#             # pull performance info from christina's reduced file\n",
    "#             s1 = perf_df[(perf_df['feature_list_method']==nameconvert[algor])&(perf_df['cohort']==cancer)&(perf_df['datatype']==platform)]\n",
    "            \n",
    "#             if s1.shape[0]!=0:\n",
    "#                 assert s1.shape[0]==1, 'shape is {}'.format(s1.shape)\n",
    "#                 performance = list(s1['Mean'])[0]\n",
    "\n",
    "#                 # add that performance to info dictionary\n",
    "#                 info[algor][cancer][platform]['Mean_Overall_Weighted_F1']=performance\n",
    "#             # if algor wasn't ran for that combo of cancer-platform\n",
    "#             else:\n",
    "#                 info[algor][cancer][platform]['Mean_Overall_Weighted_F1'] ='NA'\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "# res = {'Cohort': [], 'Platform': [], 'Algorithm_Method':[], 'Mean_Overall_Weighted_F1':[],'Features':[]}\n",
    "\n",
    "# for algor in info.keys():\n",
    "#     for cancer in info[algor].keys():\n",
    "#         for platform in info[algor][cancer].keys():\n",
    "\n",
    "#             res['Cohort'].append(cancer)\n",
    "#             res['Platform'].append(platform)\n",
    "#             res['Algorithm_Method'].append(algor)\n",
    "#             fts_list = info[algor][cancer][platform]['fts']\n",
    "# #             fts_list = ', '.join([a.split(':')[3] + '('+ a.split(':')[4]+')' for a in fts_list])\n",
    "#             fts_list = ','.join(fts_list)\n",
    "#             res['Features'].append(fts_list)\n",
    "        \n",
    "# #             perf = float(best[cancer][nameconvert[algor]]['info_'+platform]['Max_overall_weighted_f1'])\n",
    "# #             res['Max_Overall_Weighted_F1'].append(perf)\n",
    "            \n",
    "#             perf = info[algor][cancer][platform]['Mean_Overall_Weighted_F1']\n",
    "#             res['Mean_Overall_Weighted_F1'].append(perf)      \n",
    "# summary = pd.DataFrame.from_dict(res)\n",
    "# summary = summary.sort_values(by = ['Cohort','Mean_Overall_Weighted_F1', 'Platform','Algorithm_Method'], ascending =[True,False, False, False]).reset_index(drop=True)\n",
    "\n",
    "# # temp fix to drop {'All', 'MULTI', 'OVERALL', 'TOP'} bc not ready with this info yet\n",
    "# print('TODO update so this removal step of all multi overall top is not occuring')\n",
    "# # set(summary[summary['Mean_Overall_Weighted_F1']=='NA']['Platform'])\n",
    "# summary= summary[summary['Mean_Overall_Weighted_F1']!='NA']\n",
    "# summary = summary.sort_values(by = ['Cohort','Mean_Overall_Weighted_F1', 'Platform','Algorithm_Method'], ascending =[True,False, False, False]).reset_index(drop=True)\n",
    "# summary.to_csv('../data/table_docker_info/table_models.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcfb4eb",
   "metadata": {},
   "source": [
    "# Choice 2: sanity check christina's table and but use info from my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move into once tableN_docker models notebook runs (this code does a sanity check with my work)\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "extra_options = {\n",
    "    'CF':['All', 'OVERALL'],\n",
    "    'AKLIMATE':[ 'MULTI', 'TOP'], \n",
    "    'skgrid':[ 'OVERALL'],\n",
    "    'subSCOPE':[ 'OVERALL'],\n",
    "    'jadbio':['MULTI']\n",
    "\n",
    "}\n",
    "\n",
    "nameconvert = {\n",
    "    'aklimate':'AKLIMATE', \n",
    "    'cloudforest':'CF',\n",
    "    'jadbio':'jadbio',\n",
    "    'skgrid':'skgrid',\n",
    "    'subscope':'subSCOPE'\n",
    "}\n",
    "\n",
    "with open('../data/table_docker_info/options_extended_100ftmax.yml', 'r') as fh2:\n",
    "# with open('../data/table_docker_info/options_extended.yml', 'r') as fh2:\n",
    "    best = yaml.load(fh2, Loader=yaml.Loader)\n",
    "# sanity check my filtering matches christina's\n",
    "# read in christina data \n",
    "perf_df =pd.read_csv('../src/BestModelPerDataTypePerGroup_deduplicated_2022_06_16_fixed_fromChristina.txt', sep='\\t')\n",
    "\n",
    "debug_issues = []\n",
    "\n",
    "for algor in extra_options.keys():\n",
    "    for cancer in list(best.keys()):\n",
    "    #     print(cancer)\n",
    "        check= {}\n",
    "        for k,v in best[cancer][algor].items():\n",
    "            if 'info' in k:\n",
    "                # aka don't count the nans\n",
    "                if type(v['Mean_overall_weighted_f1'])!=float:\n",
    "                    # ignore the TOP and OVERALL (will calculate this later)\n",
    "                    if k.split('_')[1] != 'TOP' and k.split('_')[1]!='OVERALL':\n",
    "                        check[k.split('_')[1]]=v['Mean_overall_weighted_f1']\n",
    "\n",
    "        for check_plat in check.keys():\n",
    "            # alimate, jabio reports MULTI as ALL in christinas table\n",
    "            # CF reports All as ALL in christinas table\n",
    "            if check_plat == 'MULTI' or check_plat == 'All': \n",
    "#                 print('MULTI for ', algor)\n",
    "                s1 = perf_df[(perf_df['cohort']==cancer)&(perf_df['feature_list_method']==algor)][['featureID','cohort','performance_metric','Mean','Std','feature_list_method', 'datatype']]\n",
    "                perf_df_Mean = list(s1[s1['datatype']=='ALL']['Mean'])[0]\n",
    "                # debug\n",
    "                if perf_df_Mean != check[check_plat]:\n",
    "                    debug_issues.append('issue with not matchingup for {} perfdf mean and {} christinas mean. {} {} {}'.format(perf_df_Mean, check[check_plat], cancer, algor, check_plat)  )      \n",
    "            else:\n",
    "    #             print('checking')\n",
    "                s1 = perf_df[(perf_df['cohort']==cancer)&(perf_df['feature_list_method']==algor)][['featureID','cohort','performance_metric','Mean','Std','feature_list_method', 'datatype']]\n",
    "                perf_df_Mean = list(s1[s1['datatype']==check_plat]['Mean'])[0]\n",
    "                #debug\n",
    "                if perf_df_Mean != check[check_plat]:\n",
    "                    debug_issues.append('issue with not matchingup for {} perfdf mean and {} christinas mean. {} {} {}'.format(perf_df_Mean, check[check_plat],  cancer, algor, check_plat) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829468d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #### delete this code block\n",
    "# import pandas as pd\n",
    "# import yaml\n",
    "\n",
    "# extra_options = {\n",
    "#     'CF':['All', 'OVERALL'],\n",
    "#     'AKLIMATE':[ 'MULTI', 'TOP'], \n",
    "#     'skgrid':[ 'OVERALL'],\n",
    "#     'subSCOPE':[ 'OVERALL'],\n",
    "#     'jadbio':['MULTI']\n",
    "\n",
    "# }\n",
    "\n",
    "# nameconvert = {\n",
    "#     'aklimate':'AKLIMATE', \n",
    "#     'cloudforest':'CF',\n",
    "#     'jadbio':'jadbio',\n",
    "#     'skgrid':'skgrid',\n",
    "#     'subscope':'subSCOPE'\n",
    "# }\n",
    "\n",
    "\n",
    "# with open('../data/table_docker_info/options_extended.yml', 'r') as fh2:\n",
    "#     best = yaml.load(fh2, Loader=yaml.Loader)\n",
    "# # sanity check my filtering matches christina's\n",
    "# # read in christina data \n",
    "# perf_df =pd.read_csv('../src/BestModelPerDataTypePerGroup_deduplicated_2022_06_16_fixed_fromChristina.txt', sep='\\t')\n",
    "\n",
    "# debug_issues = []\n",
    "\n",
    "# look at the specific debug_issue manually- christina's file - she shoes 0.529\t for acc aklimate cnvr\n",
    "perf_df[(perf_df['cohort']=='ACC')&(perf_df['feature_list_method']=='AKLIMATE')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # look at specific deubg_issue manually - big results file \n",
    "# df = pd.read_csv('../src/classifier_metrics_20220511/big_results_matrix.tsv', sep='\\t', low_memory=False)\n",
    "# Grabs all models for: method, cancer, overall_weighted_f1\n",
    "subset = df[df['feature_list_method'] == 'AKLIMATE']\n",
    "subset = subset[subset['cohort'] == 'ACC']\n",
    "subset = subset[subset['performance_metric'] == 'overall_weighted_f1'].reset_index(drop=True)\n",
    "\n",
    "## uncomment out this if want to set a ftset size max##\n",
    "#         # filter for max ft size\n",
    "#         max_ft_size = int(filters)\n",
    "#         subset = subset[subset['total_features'] <= max_ft_size].reset_index(drop=True)\n",
    "subset = subset.sort_values(by='Mean', ascending=False)\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98befd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('issues were')\n",
    "for a in debug_issues:\n",
    "    print(a)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35ae6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e45b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b47d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c483781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4305035",
   "metadata": {},
   "source": [
    "### sanity check\n",
    "\n",
    "AssertionError: issue with not matchingup for 0.529 perfdf mean and 0.553 christinas mean. ACC AKLIMATE CNVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166327e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmatrix_raw= pd.read_csv('../src/classifier_metrics_20220511/big_results_matrix.tsv', sep='\\t', low_memory=False)\n",
    "\n",
    "#####\n",
    "grp = 'AKLIMATE'\n",
    "c = 'ACC'\n",
    "p = 'CNVR'\n",
    "#####\n",
    "bmatrix = bmatrix_raw[bmatrix_raw['feature_list_method'] == grp]\n",
    "bmatrix = bmatrix[bmatrix['cohort'] == c]\n",
    "bmatrix = bmatrix[bmatrix['performance_metric'] == pmetric].reset_index(drop=True)\n",
    "bmatrix = bmatrix.sort_values(by='Mean', ascending=False)\n",
    "ms = [a for a in bmatrix['model'] if p in a]\n",
    "bmatrix[bmatrix['model'].isin(ms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmatrix_raw[bmatrix_raw['total_features']!= '__NO_LIST__'].reset_index(drop=True).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfebb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmatrix_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ddfab",
   "metadata": {},
   "source": [
    "### end of sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb8c47",
   "metadata": {},
   "source": [
    "### WIP - continue with adding in the OVERALL category for that algor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/table_docker_info/options_extended.yml', 'r') as fh2:\n",
    "    best = yaml.load(fh2, Loader=yaml.Loader)\n",
    "\n",
    "# determine which algors need to calculate overall\n",
    "models_need_overall= []\n",
    "for k,v in extra_options.items():\n",
    "    if 'OVERALL' not in v:\n",
    "        models_need_overall.append(k)\n",
    "# add in OVERALL = highest performance of all models from that algor\n",
    "# thus will say which model of AKLIAMTE is OVERALL, then for skgrid, etc\n",
    "for algor in models_need_overall:\n",
    "    for cancer in list(best.keys()):\n",
    "        # find the model \n",
    "        overall_4_algor = {'model':'NA', 'score': 0}\n",
    "        for KEY in [a for a in best[cancer][algor].keys() if 'info' in a]:\n",
    "            perform = best[cancer][algor][KEY]['Mean_overall_weighted_f1']\n",
    "            if perform > overall_4_algor['score']:\n",
    "                overall_4_algor['score']=perform\n",
    "                overall_4_algor['model']=KEY\n",
    "        best[cancer][algor]['OVERALL']= best[cancer][algor][KEY.split('_')[1]]\n",
    "        best[cancer][algor]['info_OVERALL']= best[cancer][algor][overall_4_algor['model']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b5aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull up info used in docker models\n",
    "with open('../../../09_docker/gdan-tmp-models/tools/model_info.json', 'r') as file:\n",
    "    info = yaml.safe_load(file)\n",
    "\n",
    "reverse_name = {\n",
    "    'AKLIMATE':'aklimate', \n",
    "    'CF':'cloudforest',\n",
    "    'jadbio':'jadbio',\n",
    "    'skgrid':'skgrid',\n",
    "    'subSCOPE':'subscope'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead86b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all info into 1 df\n",
    "# #####\n",
    "algor = 'CF'\n",
    "cancer = 'BRCA'\n",
    "# #####\n",
    "# skgrid\n",
    "\n",
    "res = {'Command':[], 'Mean_Overall_Weighted_F1':[],'Std_overall_weighted_f1':[], 'Cohort': [], 'Platform': [], 'Algorithm_Method':[],'Features':[]}\n",
    "for cancer in best.keys():\n",
    "    for algor in best[cancer].keys():\n",
    "        \n",
    "        platform_cyles = [a for a in best[cancer][algor] if 'info' in a]\n",
    "        # update the above\n",
    "        \n",
    "        for pc in platform_cyles:\n",
    "            platform = pc.split('_')[1]\n",
    "            # skip when don't have model info on that \n",
    "            if 'NO_MODEL' not in best[cancer][algor][platform]:\n",
    "                res['Command'].append('bash RUN_model.sh {} {} {} YOUR-DATA.tsv'.format(cancer, platform, reverse_name[algor]))\n",
    "\n",
    "                mean_value = best[cancer][algor][pc]['Mean_overall_weighted_f1']\n",
    "                res['Mean_Overall_Weighted_F1'].append(mean_value)\n",
    "\n",
    "                sd_value = best[cancer][algor][pc]['Std_overall_weighted_f1']\n",
    "                res['Std_overall_weighted_f1'].append(sd_value)\n",
    "\n",
    "                res['Cohort'].append(cancer)\n",
    "\n",
    "                # modify in table TOP to OVERALL but command col will use TOP still\n",
    "                if platform == 'TOP':\n",
    "                    res['Platform'].append('OVERALL')\n",
    "                elif platform == 'All':\n",
    "                    res['Platform'].append('MULTI')\n",
    "                else:\n",
    "                    res['Platform'].append(platform)\n",
    "\n",
    "                res['Algorithm_Method'].append(reverse_name[algor])\n",
    "\n",
    "                fts = info[reverse_name[algor]][cancer][platform]['fts']\n",
    "                res['Features'].append(fts)\n",
    "            else:\n",
    "                res['Command'].append('NA')\n",
    "\n",
    "                res['Mean_Overall_Weighted_F1'].append('NA')\n",
    "\n",
    "                res['Std_overall_weighted_f1'].append('NA')\n",
    "\n",
    "                res['Cohort'].append(cancer)\n",
    "\n",
    "                # modify in table TOP to OVERALL but command col will use TOP still\n",
    "                if platform == 'TOP':\n",
    "                    res['Platform'].append('OVERALL')\n",
    "                elif platform == 'All':\n",
    "                    res['Platform'].append('MULTI')\n",
    "                else:\n",
    "                    res['Platform'].append(platform)\n",
    "\n",
    "                res['Algorithm_Method'].append(reverse_name[algor])\n",
    "\n",
    "                res['Features'].append('NA')                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame.from_dict(res)\n",
    "summary = summary.sort_values(by = ['Mean_Overall_Weighted_F1','Cohort', 'Platform','Algorithm_Method'], ascending =[False, False,False, False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6bca5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
